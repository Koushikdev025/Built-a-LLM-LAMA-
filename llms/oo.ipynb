{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Corpus:\n",
      "This is the first document.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n",
      "Initial Vocabulary:\n",
      "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
      "Vocabulary Size: 20\n",
      "\n",
      "Pre-tokenized Word Frequencies:\n",
      "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "\n",
      "--- Starting BPE Merges ---\n",
      "Initial Splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 1/15\n",
      "Top 5 Pair Frequencies: [(('s', '</w>'), 8), (('i', 's'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4)]\n",
      "Found Best Pair: ('s', '</w>') with Frequency: 8\n",
      "Merging ('s', '</w>') into 's</w>'\n",
      "Splits after merge: {('T', 'h', 'i', 's</w>'): 2, ('i', 's</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 2/15\n",
      "Top 5 Pair Frequencies: [(('i', 's</w>'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4), (('e', '</w>'), 4)]\n",
      "Found Best Pair: ('i', 's</w>') with Frequency: 7\n",
      "Merging ('i', 's</w>') into 'is</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'is</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 3/15\n",
      "Top 5 Pair Frequencies: [(('t', 'h'), 7), (('h', 'is</w>'), 4), (('h', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4)]\n",
      "Found Best Pair: ('t', 'h') with Frequency: 7\n",
      "Merging ('t', 'h') into 'th'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('th', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 4/15\n",
      "Top 5 Pair Frequencies: [(('th', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4)]\n",
      "Found Best Pair: ('th', 'e') with Frequency: 4\n",
      "Merging ('th', 'e') into 'the'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 5/15\n",
      "Top 5 Pair Frequencies: [(('the', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4)]\n",
      "Found Best Pair: ('the', '</w>') with Frequency: 4\n",
      "Merging ('the', '</w>') into 'the</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 6/15\n",
      "Top 5 Pair Frequencies: [(('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4)]\n",
      "Found Best Pair: ('d', 'o') with Frequency: 4\n",
      "Merging ('d', 'o') into 'do'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('do', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 7/15\n",
      "Top 5 Pair Frequencies: [(('do', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4)]\n",
      "Found Best Pair: ('do', 'c') with Frequency: 4\n",
      "Merging ('do', 'c') into 'doc'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('doc', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 8/15\n",
      "Top 5 Pair Frequencies: [(('doc', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4)]\n",
      "Found Best Pair: ('doc', 'u') with Frequency: 4\n",
      "Merging ('doc', 'u') into 'docu'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docu', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 9/15\n",
      "Top 5 Pair Frequencies: [(('docu', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3)]\n",
      "Found Best Pair: ('docu', 'm') with Frequency: 4\n",
      "Merging ('docu', 'm') into 'docum'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docum', 'e', 'n', 't', '.', '</w>'): 2, ('docum', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docum', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 10/15\n",
      "Top 5 Pair Frequencies: [(('docum', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3)]\n",
      "Found Best Pair: ('docum', 'e') with Frequency: 4\n",
      "Merging ('docum', 'e') into 'docume'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docume', 'n', 't', '.', '</w>'): 2, ('docume', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docume', 'n', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 11/15\n",
      "Top 5 Pair Frequencies: [(('docume', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3)]\n",
      "Found Best Pair: ('docume', 'n') with Frequency: 4\n",
      "Merging ('docume', 'n') into 'documen'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('documen', 't', '.', '</w>'): 2, ('documen', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('documen', 't', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 12/15\n",
      "Top 5 Pair Frequencies: [(('documen', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3)]\n",
      "Found Best Pair: ('documen', 't') with Frequency: 4\n",
      "Merging ('documen', 't') into 'document'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 13/15\n",
      "Top 5 Pair Frequencies: [(('i', 'r'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2)]\n",
      "Found Best Pair: ('i', 'r') with Frequency: 3\n",
      "Merging ('i', 'r') into 'ir'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 14/15\n",
      "Top 5 Pair Frequencies: [(('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2)]\n",
      "Found Best Pair: ('.', '</w>') with Frequency: 3\n",
      "Merging ('.', '</w>') into '.</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>'}\n",
      "------------------------------\n",
      "\n",
      "Merge Iteration 15/15\n",
      "Top 5 Pair Frequencies: [(('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2), (('ir', 's'), 2)]\n",
      "Found Best Pair: ('d', '</w>') with Frequency: 3\n",
      "Merging ('d', '</w>') into 'd</w>'\n",
      "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>']\n",
      "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>', ('d', '</w>'): 'd</w>'}\n",
      "------------------------------\n",
      "\n",
      "--- BPE Merges Complete ---\n",
      "Final Vocabulary Size: 35\n",
      "\n",
      "Learned Merges (Pair -> New Token):\n",
      "('s', '</w>') -> 's</w>'\n",
      "('i', 's</w>') -> 'is</w>'\n",
      "('t', 'h') -> 'th'\n",
      "('th', 'e') -> 'the'\n",
      "('the', '</w>') -> 'the</w>'\n",
      "('d', 'o') -> 'do'\n",
      "('do', 'c') -> 'doc'\n",
      "('doc', 'u') -> 'docu'\n",
      "('docu', 'm') -> 'docum'\n",
      "('docum', 'e') -> 'docume'\n",
      "('docume', 'n') -> 'documen'\n",
      "('documen', 't') -> 'document'\n",
      "('i', 'r') -> 'ir'\n",
      "('.', '</w>') -> '.</w>'\n",
      "('d', '</w>') -> 'd</w>'\n",
      "\n",
      "Final Word Splits after all merges:\n",
      "{('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
      "\n",
      "Final Vocabulary (sorted):\n",
      "[' ', '.', '.</w>', '</w>', '?', 'A', 'I', 'T', 'c', 'd', 'd</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'e', 'f', 'h', 'i', 'ir', 'is</w>', 'm', 'n', 'o', 'r', 's', 's</w>', 't', 'th', 'the', 'the</w>', 'u']\n"
     ]
    }
   ],
   "source": [
    "# VS Code Requirement: You will almost certainly have this already installed in your VS Code or Cursor, so no need to do anything\n",
    "# - The Microsoft Python extension (ms-python.python)\n",
    "# This extension provides the ability to run '%%' cells interactively.\n",
    "# Below you should see \"Run Cell\" and \"Run Below\" that is written just above '# %% [markdown]' line\n",
    "\n",
    "# %% [markdown]\n",
    "# # Building a Byte Pair Encoding (BPE) Tokenizer from Scratch\n",
    "#\n",
    "# This tutorial walks through the process of creating a basic BPE tokenizer, a common type of tokenizer used in Large Language Models (LLMs).\n",
    "#\n",
    "# ## Step 1: Prepare Training Data\n",
    "#\n",
    "# The first step in building any tokenizer is to have a corpus of text to train it on. The tokenizer learns merge rules based on the frequency of character pairs in this data.\n",
    "#\n",
    "# i: 1\n",
    "#\n",
    "# s: 2\n",
    "#\n",
    "# is: 3\n",
    "#\n",
    "# Even though \"i\" and \"s\" are separate tokens, we create a new token \"is\" by merging them as they frequently appear together (is, this, his, miss, dismiss, list, fist, twist, mist, whisk, visible, vision, revise, crisis), reducing computation needs by 2x at any place where we merge those 2 tokens. This is how we will itteratively merge most frequent pairs. The new tokens can also be further merged.\n",
    "#\n",
    "# Let's start with a small example corpus.\n",
    "\n",
    "# %%\n",
    "# Our sample training data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "print(\"Training Corpus:\")\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2: Initialize Vocabulary and Pre-tokenize\n",
    "#\n",
    "# The BPE algorithm starts with a base vocabulary consisting of all unique characters present in the training data.\n",
    "#\n",
    "# We also need to pre-tokenize the corpus. This usually involves splitting the text into words (or word-like units) and then representing each word as a sequence of its individual characters. We often add a special end-of-word token (like `</w>`) to mark word boundaries, which helps the tokenizer learn subword units that align better with whole words.\n",
    "\n",
    "# %%\n",
    "# Initialize vocabulary with unique characters\n",
    "unique_chars = set()\n",
    "for doc in corpus:\n",
    "    for char in doc:\n",
    "        unique_chars.add(char)\n",
    "\n",
    "vocab = list(unique_chars)\n",
    "vocab.sort() # For consistent order of characters, making the vocabulary list predictable\n",
    "\n",
    "# Add a special end-of-word token\n",
    "end_of_word = \"</w>\"\n",
    "vocab.append(end_of_word)\n",
    "\n",
    "print(\"Initial Vocabulary:\")\n",
    "print(vocab)\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "# Pre-tokenize the corpus: Split into words and then characters\n",
    "# We'll split by space for simplicity and add the end-of-word token\n",
    "word_splits = {}\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word:\n",
    "            char_list = list(word) + [end_of_word]\n",
    "            # Use tuple for immutability if storing counts later - you can't change tuple once it's created (values, order, adding, removing elements, etc.), so they can be used as dictionary keys because of that.\n",
    "            word_tuple = tuple(char_list)\n",
    "            if word_tuple not in word_splits:\n",
    "                 word_splits[word_tuple] = 0\n",
    "            word_splits[word_tuple] += 1 # Count frequency of each initial word split\n",
    "\n",
    "print(\"\\nPre-tokenized Word Frequencies:\")\n",
    "print(word_splits)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Helper Function: `get_pair_stats`\n",
    "#\n",
    "# This function takes the current word splits (represented as a dictionary where keys are tuples of symbols/characters forming a word and values are their frequencies) and calculates the frequency of each adjacent pair of symbols across the entire corpus.\n",
    "#\n",
    "# **Input Example (`splits`):**\n",
    "# ```\n",
    "# {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...}\n",
    "# ```\n",
    "# **Output Example (`pair_counts`):**\n",
    "# ```\n",
    "# {('i', 's'): 4, ('s', '</w>'): 4, ('T', 'h'): 2, ...}\n",
    "# ```\n",
    "\n",
    "# %%\n",
    "import collections\n",
    "\n",
    "def get_pair_stats(splits):\n",
    "    \"\"\"Counts the frequency of adjacent pairs in the word splits.\"\"\"\n",
    "    # Initialize a dictionary with default values of 0 to count pairs of symbols.\n",
    "    # defaultdict: It's like a regular dictionary (dict), but with a key difference.\n",
    "    # If you try to access or modify a key that doesn't exist, instead of raising a KeyError,\n",
    "    # it automatically creates that key and assigns it a default value.\n",
    "    # int: This is the \"default factory\" you provide when creating the defaultdict. When a new key is created, it needs a default value, defaultdict calls this factory function. int() called with no arguments returns 0.\n",
    "    pair_counts = collections.defaultdict(int)\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pair_counts[pair] += freq # Add the frequency of the word to the pair count\n",
    "    return pair_counts\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Helper Function: `merge_pair`\n",
    "#\n",
    "# This function takes a specific pair (`pair_to_merge`) that we want to combine and the current `splits`. It iterates through all the word representations in `splits`, replaces occurrences of the `pair_to_merge` with a new single token (concatenation of the pair), and returns the updated `splits`.\n",
    "#\n",
    "# **Input Example:**\n",
    "# - `pair_to_merge`: `('i', 's')`\n",
    "# - `splits`: `{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...}`\n",
    "#\n",
    "# **Output Example (`new_splits`):**\n",
    "# - `{('T', 'h', 'is', '</w>'): 2, ('is', '</w>'): 2, ...}` (assuming 'is' is the merged token)\n",
    "\n",
    "# %%\n",
    "def merge_pair(pair_to_merge, splits):\n",
    "    \"\"\"Merges the specified pair in the word splits.\"\"\"\n",
    "    new_splits = {}\n",
    "    (first, second) = pair_to_merge\n",
    "    merged_token = first + second\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            # If the current and next symbol match the pair to merge\n",
    "            if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                new_symbols.append(merged_token)\n",
    "                i += 2 # Skip the next symbol\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_splits[tuple(new_symbols)] = freq # Use the updated symbol list as the key\n",
    "    return new_splits\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Step 3: Iterative BPE Merging Loop\n",
    "#\n",
    "# Now we perform the core BPE training. We'll loop for a fixed number of merges (`num_merges`). In each iteration:\n",
    "# 1. Calculate the frequencies of all adjacent pairs in the current word representations using `get_pair_stats`.\n",
    "# 2. Find the pair with the highest frequency (`best_pair`).\n",
    "# 3. Merge this `best_pair` across all word representations using `merge_pair`.\n",
    "# 4. Add the newly formed token (concatenation of `best_pair`) to our vocabulary (`vocab`).\n",
    "# 5. Store the merge rule (mapping the pair to the new token) in the `merges` dictionary.\n",
    "#\n",
    "# We'll add print statements to observe the state at each step of the loop.\n",
    "\n",
    "# %%\n",
    "# --- BPE Training Loop Initialization ---\n",
    "num_merges = 15\n",
    "# Stores merge rules, e.g., {('a', 'b'): 'ab'}\n",
    "# Example: {('T', 'h'): 'Th'}\n",
    "merges = {}\n",
    "# Initial word splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 2, ...}\n",
    "current_splits = word_splits.copy() # Start with initial word splits\n",
    "\n",
    "print(\"\\n--- Starting BPE Merges ---\")\n",
    "print(f\"Initial Splits: {current_splits}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(f\"\\nMerge Iteration {i+1}/{num_merges}\")\n",
    "\n",
    "    # 1. Calculate Pair Frequencies\n",
    "    pair_stats = get_pair_stats(current_splits)\n",
    "    if not pair_stats:\n",
    "        print(\"No more pairs to merge.\")\n",
    "        break\n",
    "    # Optional: Print top 5 pairs for inspection\n",
    "    sorted_pairs = sorted(pair_stats.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(f\"Top 5 Pair Frequencies: {sorted_pairs[:5]}\")\n",
    "\n",
    "    # 2. Find Best Pair\n",
    "    # The 'max' function iterates over all key-value pairs in the 'pair_stats' dictionary\n",
    "    # The 'key=pair_stats.get' tells 'max' to use the frequency (value) for comparison, not the pair (key) itself\n",
    "    # This way, 'max' selects the pair with the highest frequency\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    best_freq = pair_stats[best_pair]\n",
    "    print(f\"Found Best Pair: {best_pair} with Frequency: {best_freq}\")\n",
    "\n",
    "    # 3. Merge the Best Pair\n",
    "    current_splits = merge_pair(best_pair, current_splits)\n",
    "    new_token = best_pair[0] + best_pair[1]\n",
    "    print(f\"Merging {best_pair} into '{new_token}'\")\n",
    "    print(f\"Splits after merge: {current_splits}\")\n",
    "\n",
    "    # 4. Update Vocabulary\n",
    "    vocab.append(new_token)\n",
    "    print(f\"Updated Vocabulary: {vocab}\")\n",
    "\n",
    "    # 5. Store Merge Rule\n",
    "    merges[best_pair] = new_token\n",
    "    print(f\"Updated Merges: {merges}\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Step 4: Review Final Results\n",
    "#\n",
    "# After the loop finishes, we can examine the final state:\n",
    "# - The learned merge rules (`merges`).\n",
    "# - The final representation of words after merges (`current_splits`).\n",
    "# - The complete vocabulary (`vocab`) containing initial characters and learned subword tokens.\n",
    "\n",
    "# %%\n",
    "# --- BPE Merges Complete ---\n",
    "print(\"\\n--- BPE Merges Complete ---\")\n",
    "print(f\"Final Vocabulary Size: {len(vocab)}\")\n",
    "print(\"\\nLearned Merges (Pair -> New Token):\")\n",
    "# Pretty print merges\n",
    "for pair, token in merges.items():\n",
    "    print(f\"{pair} -> '{token}'\")\n",
    "\n",
    "print(\"\\nFinal Word Splits after all merges:\")\n",
    "print(current_splits)\n",
    "\n",
    "print(\"\\nFinal Vocabulary (sorted):\")\n",
    "# Sort for consistent viewing\n",
    "final_vocab_sorted = sorted(list(set(vocab))) # Use set to remove potential duplicates if any step introduced them\n",
    "print(final_vocab_sorted)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  hidden_size: 128\n",
      "  intermediate_size: 352 (Calculated from ratio 2.67, multiple of 32)\n",
      "  hidden_act: silu\n",
      "  rms_norm_eps: 1e-05\n",
      "\n",
      "Sample Input Shape (Before FFN Block Norm):\n",
      "  input_to_ffn_block: torch.Size([2, 10, 128])\n",
      "Shape after Post-Attention RMSNorm:\n",
      "  normalized_hidden_states: torch.Size([2, 10, 128])\n",
      "\n",
      "Shapes within FFN:\n",
      "  gate_output: torch.Size([2, 10, 352])\n",
      "  up_output: torch.Size([2, 10, 352])\n",
      "  gated_result: torch.Size([2, 10, 352])\n",
      "  ffn_output: torch.Size([2, 10, 128])\n",
      "\n",
      "Shape after FFN Residual Connection:\n",
      "  final_output: torch.Size([2, 10, 128])\n",
      "\n",
      "Output shape from simplified FFN module (before residual): torch.Size([2, 10, 128])\n",
      "Output shape after external residual connection: torch.Size([2, 10, 128])\n",
      "Outputs are close: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lesson_4_llama4_feedforward_code.py\n",
    "\n",
    "# %% [markdown]\n",
    "# # Understanding the Llama 4 Feed-Forward Network (FFN)\n",
    "#\n",
    "# This tutorial explores the Feed-Forward Network (FFN) used in the Llama 4 architecture, specifically the MLP (Multi-Layer Perceptron) variant used in dense layers. The FFN is applied independently to each token position after the attention mechanism and residual connection. Its role is to further process the information aggregated by the attention layer, adding non-linearity and increasing the model's representational capacity.\n",
    "#\n",
    "# Llama models typically use a specific FFN structure involving gated linear units (like SwiGLU), which has shown strong performance. We will break down the `Llama4TextMLP` module and its surrounding components (like Layer Normalization) step by step.\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Setup and Configuration\n",
    "#\n",
    "# First, let's define configuration parameters relevant to the FFN and create sample input data. This input data represents the hidden state *after* the attention block and its residual connection, but *before* the post-attention layer normalization.\n",
    "\n",
    "# %%\n",
    "# Configuration (Simplified for clarity)\n",
    "hidden_size = 128  # Dimensionality of the model's hidden states\n",
    "# Intermediate size for the FFN. Often calculated based on hidden_size.\n",
    "# A common pattern is around 2.67 * hidden_size, rounded up to a multiple (e.g., 256).\n",
    "ffn_intermediate_ratio = 8 / 3\n",
    "multiple_of = 32  # Common multiple for FFN intermediate size\n",
    "intermediate_size = int(hidden_size * ffn_intermediate_ratio)\n",
    "# This line of code adjusts the intermediate_size to be a multiple of 'multiple_of'.\n",
    "# It does this by first adding 'multiple_of - 1' to 'intermediate_size', then performing integer division by 'multiple_of',\n",
    "# and finally multiplying the result by 'multiple_of'. This effectively rounds up 'intermediate_size' to the nearest multiple of 'multiple_of'.\n",
    "intermediate_size = ((intermediate_size + multiple_of -\n",
    "                     1) // multiple_of) * multiple_of\n",
    "\n",
    "hidden_act = \"silu\"  # Activation function (SiLU/Swish)\n",
    "rms_norm_eps = 1e-5  # Epsilon for RMSNorm\n",
    "ffn_bias = False  # Whether to use bias in FFN linear layers\n",
    "\n",
    "# Sample Input (Represents output of Attention + Residual)\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "# This is the state before the post-attention LayerNorm\n",
    "input_to_ffn_block = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(\n",
    "    f\"  intermediate_size: {intermediate_size} (Calculated from ratio {ffn_intermediate_ratio:.2f}, multiple of {multiple_of})\")\n",
    "print(f\"  hidden_act: {hidden_act}\")\n",
    "print(f\"  rms_norm_eps: {rms_norm_eps}\")\n",
    "\n",
    "print(\"\\nSample Input Shape (Before FFN Block Norm):\")\n",
    "print(f\"  input_to_ffn_block: {input_to_ffn_block.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2: Pre-Normalization (Post-Attention LayerNorm)\n",
    "#\n",
    "# Before passing the hidden state through the FFN, Llama applies a Layer Normalization step. Unlike standard Transformers that often use LayerNorm *after* the FFN and residual connection, Llama uses a pre-normalization approach. Here, it's the \"post-attention\" normalization (`post_attention_layernorm` in the original `Llama4TextDecoderLayer`). Llama typically uses RMSNorm.\n",
    "\n",
    "# %%\n",
    "# Simplified RMSNorm Implementation\n",
    "\n",
    "\n",
    "class SimplifiedRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # Learnable gain parameter\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        # Calculate in float32 for stability\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        # Calculate variance (mean of squares) across the hidden dimension\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        # Normalize: input / sqrt(variance + epsilon)\n",
    "        hidden_states = hidden_states * \\\n",
    "            torch.rsqrt(variance + self.variance_epsilon)\n",
    "        # Apply learnable weight and cast back to original dtype\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "\n",
    "# Instantiate and apply the normalization\n",
    "post_attention_norm = SimplifiedRMSNorm(hidden_size, eps=rms_norm_eps)\n",
    "normalized_hidden_states = post_attention_norm(input_to_ffn_block)\n",
    "\n",
    "print(\"Shape after Post-Attention RMSNorm:\")\n",
    "print(f\"  normalized_hidden_states: {normalized_hidden_states.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: The Feed-Forward Network (MLP with Gated Linear Unit)\n",
    "#\n",
    "# The core of the FFN in Llama's dense layers is an MLP using a gated mechanism, often referred to as SwiGLU (SiLU Gated Linear Unit). It consists of three linear projections:\n",
    "#\n",
    "# 1.  **`gate_proj`:** Projects the input to the `intermediate_size`.\n",
    "# 2.  **`up_proj`:** Also projects the input to the `intermediate_size`.\n",
    "# 3.  **`down_proj`:** Projects the result back down to the `hidden_size`.\n",
    "#\n",
    "# The calculation is: `down_proj( F.silu(gate_proj(x)) * up_proj(x) )`\n",
    "#\n",
    "# - The `gate_proj` output is passed through an activation function (SiLU/Swish).\n",
    "# - This activated gate is element-wise multiplied by the `up_proj` output.\n",
    "# - The result is then projected back to the original hidden dimension by `down_proj`.\n",
    "\n",
    "# %%\n",
    "# Define FFN layers\n",
    "gate_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "up_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "down_proj = nn.Linear(intermediate_size, hidden_size, bias=ffn_bias)\n",
    "\n",
    "# Define the activation function (SiLU/Swish)\n",
    "# ACT2FN could be used here, but for simplicity, we directly use nn.SiLU\n",
    "if hidden_act == \"silu\":\n",
    "    activation_fn = nn.SiLU()\n",
    "else:\n",
    "    # Add other activations if needed, otherwise raise error\n",
    "    raise NotImplementedError(\n",
    "        f\"Activation {hidden_act} not implemented in this example.\")\n",
    "\n",
    "# Apply the FFN layers to the *normalized* hidden states\n",
    "gate_output = gate_proj(normalized_hidden_states)\n",
    "up_output = up_proj(normalized_hidden_states)\n",
    "\n",
    "# Apply activation to the gate and perform element-wise multiplication\n",
    "activated_gate = activation_fn(gate_output)\n",
    "gated_result = activated_gate * up_output\n",
    "\n",
    "# Apply the final down projection\n",
    "ffn_output = down_proj(gated_result)\n",
    "\n",
    "print(\"\\nShapes within FFN:\")\n",
    "# (batch, seq_len, intermediate_size)\n",
    "print(f\"  gate_output: {gate_output.shape}\")\n",
    "# (batch, seq_len, intermediate_size)\n",
    "print(f\"  up_output: {up_output.shape}\")\n",
    "# (batch, seq_len, intermediate_size)\n",
    "print(f\"  gated_result: {gated_result.shape}\")\n",
    "print(f\"  ffn_output: {ffn_output.shape}\")   # (batch, seq_len, hidden_size)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4: Residual Connection\n",
    "#\n",
    "# Similar to the attention block, a residual connection is used around the FFN block. The output of the FFN (`ffn_output`) is added to the input that went *into* the FFN block (i.e., the output of the attention block + its residual, stored here as `input_to_ffn_block`).\n",
    "\n",
    "# %%\n",
    "# Add the FFN output to the input of the FFN block (before normalization)\n",
    "final_output = input_to_ffn_block + ffn_output\n",
    "\n",
    "print(\"\\nShape after FFN Residual Connection:\")\n",
    "# Should be (batch, seq_len, hidden_size)\n",
    "print(f\"  final_output: {final_output.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Putting it Together (Simplified Llama4 FFN Block)\n",
    "#\n",
    "# Let's combine the normalization and MLP steps into a simplified module. Note that the residual connection is typically handled *outside* this specific module in the main `DecoderLayer`.\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "class SimplifiedLlama4FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.hidden_act = config['hidden_act']\n",
    "        self.ffn_bias = config['ffn_bias']\n",
    "        self.rms_norm_eps = config['rms_norm_eps']\n",
    "\n",
    "        # Normalization Layer (applied before MLP)\n",
    "        self.norm = SimplifiedRMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n",
    "\n",
    "        # MLP Layers\n",
    "        self.gate_proj = nn.Linear(\n",
    "            self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.up_proj = nn.Linear(\n",
    "            self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.down_proj = nn.Linear(\n",
    "            self.intermediate_size, self.hidden_size, bias=self.ffn_bias)\n",
    "\n",
    "        # Activation\n",
    "        if self.hidden_act == \"silu\":\n",
    "            self.activation_fn = nn.SiLU()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Activation {self.hidden_act} not implemented.\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 1. Apply pre-FFN normalization\n",
    "        normalized_states = self.norm(hidden_states)\n",
    "\n",
    "        # 2. Apply MLP (SwiGLU)\n",
    "        gate = self.gate_proj(normalized_states)\n",
    "        up = self.up_proj(normalized_states)\n",
    "        down = self.down_proj(self.activation_fn(gate) * up)\n",
    "\n",
    "        # This module returns *only* the MLP output.\n",
    "        # The residual connection is applied outside.\n",
    "        return down\n",
    "\n",
    "\n",
    "# Instantiate and run the simplified module\n",
    "ffn_config_dict = {\n",
    "    'hidden_size': hidden_size,\n",
    "    'intermediate_size': intermediate_size,\n",
    "    'hidden_act': hidden_act,\n",
    "    'ffn_bias': ffn_bias,\n",
    "    'rms_norm_eps': rms_norm_eps,\n",
    "}\n",
    "\n",
    "simplified_ffn_module = SimplifiedLlama4FFN(ffn_config_dict)\n",
    "\n",
    "# Run forward pass using the module\n",
    "# Input is the state *before* the norm\n",
    "mlp_output_from_module = simplified_ffn_module(input_to_ffn_block)\n",
    "\n",
    "# Apply the residual connection externally\n",
    "final_output_from_module = input_to_ffn_block + mlp_output_from_module\n",
    "\n",
    "print(\"\\nOutput shape from simplified FFN module (before residual):\",\n",
    "      mlp_output_from_module.shape)\n",
    "print(\"Output shape after external residual connection:\",\n",
    "      final_output_from_module.shape)\n",
    "# Verify that the manual calculation matches the module output (should be very close)\n",
    "print(\"Outputs are close:\", torch.allclose(\n",
    "    final_output, final_output_from_module, atol=1e-6))\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Conclusion\n",
    "#\n",
    "# The Llama 4 Feed-Forward Network block typically consists of:\n",
    "# 1.  **Pre-Normalization:** An RMSNorm layer applied to the output of the previous (attention + residual) block.\n",
    "# 2.  **Gated MLP (SwiGLU):** Two linear layers projecting to an intermediate dimension, combined using an activation (SiLU) and element-wise multiplication, followed by a projection back to the hidden dimension.\n",
    "# 3.  **Residual Connection:** The output of the MLP is added back to the input of the normalization layer.\n",
    "#\n",
    "# This structure provides the necessary non-linearity and processing power for each token position within the transformer layer.\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  hidden_size: 128\n",
      "  intermediate_size: 352 (Calculated from ratio 2.67, multiple of 32)\n",
      "  hidden_act: silu\n",
      "  rms_norm_eps: 1e-05\n",
      "\n",
      "Sample Input Shape (Before FFN Block Norm):\n",
      "  input_to_ffn_block: torch.Size([2, 10, 128])\n",
      "Shape after Post-Attention RMSNorm:\n",
      "  normalized_hidden_states: torch.Size([2, 10, 128])\n",
      "\n",
      "Shapes within FFN:\n",
      "  gate_output: torch.Size([2, 10, 352])\n",
      "  up_output: torch.Size([2, 10, 352])\n",
      "  gated_result: torch.Size([2, 10, 352])\n",
      "  ffn_output: torch.Size([2, 10, 128])\n",
      "\n",
      "Shape after FFN Residual Connection:\n",
      "  final_output: torch.Size([2, 10, 128])\n",
      "\n",
      "Output shape from simplified FFN module (before residual): torch.Size([2, 10, 128])\n",
      "Output shape after external residual connection: torch.Size([2, 10, 128])\n",
      "Outputs are close: False\n"
     ]
    }
   ],
   "source": [
    "# lesson_4_llama4_feedforward_code.py\n",
    "\n",
    "# %% [markdown]\n",
    "# # Understanding the Llama 4 Feed-Forward Network (FFN)\n",
    "#\n",
    "# This tutorial explores the Feed-Forward Network (FFN) used in the Llama 4 architecture, specifically the MLP (Multi-Layer Perceptron) variant used in dense layers. The FFN is applied independently to each token position after the attention mechanism and residual connection. Its role is to further process the information aggregated by the attention layer, adding non-linearity and increasing the model's representational capacity.\n",
    "#\n",
    "# Llama models typically use a specific FFN structure involving gated linear units (like SwiGLU), which has shown strong performance. We will break down the `Llama4TextMLP` module and its surrounding components (like Layer Normalization) step by step.\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Setup and Configuration\n",
    "#\n",
    "# First, let's define configuration parameters relevant to the FFN and create sample input data. This input data represents the hidden state *after* the attention block and its residual connection, but *before* the post-attention layer normalization.\n",
    "\n",
    "# %%\n",
    "# Configuration (Simplified for clarity)\n",
    "hidden_size = 128  # Dimensionality of the model's hidden states\n",
    "# Intermediate size for the FFN. Often calculated based on hidden_size.\n",
    "# A common pattern is around 2.67 * hidden_size, rounded up to a multiple (e.g., 256).\n",
    "ffn_intermediate_ratio = 8 / 3\n",
    "multiple_of = 32  # Common multiple for FFN intermediate size\n",
    "intermediate_size = int(hidden_size * ffn_intermediate_ratio)\n",
    "# This line of code adjusts the intermediate_size to be a multiple of 'multiple_of'.\n",
    "# It does this by first adding 'multiple_of - 1' to 'intermediate_size', then performing integer division by 'multiple_of',\n",
    "# and finally multiplying the result by 'multiple_of'. This effectively rounds up 'intermediate_size' to the nearest multiple of 'multiple_of'.\n",
    "intermediate_size = ((intermediate_size + multiple_of -\n",
    "                     1) // multiple_of) * multiple_of\n",
    "\n",
    "hidden_act = \"silu\"  # Activation function (SiLU/Swish)\n",
    "rms_norm_eps = 1e-5  # Epsilon for RMSNorm\n",
    "ffn_bias = False  # Whether to use bias in FFN linear layers\n",
    "\n",
    "# Sample Input (Represents output of Attention + Residual)\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "# This is the state before the post-attention LayerNorm\n",
    "input_to_ffn_block = torch.randn(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(\n",
    "    f\"  intermediate_size: {intermediate_size} (Calculated from ratio {ffn_intermediate_ratio:.2f}, multiple of {multiple_of})\")\n",
    "print(f\"  hidden_act: {hidden_act}\")\n",
    "print(f\"  rms_norm_eps: {rms_norm_eps}\")\n",
    "\n",
    "print(\"\\nSample Input Shape (Before FFN Block Norm):\")\n",
    "print(f\"  input_to_ffn_block: {input_to_ffn_block.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2: Pre-Normalization (Post-Attention LayerNorm)\n",
    "#\n",
    "# Before passing the hidden state through the FFN, Llama applies a Layer Normalization step. Unlike standard Transformers that often use LayerNorm *after* the FFN and residual connection, Llama uses a pre-normalization approach. Here, it's the \"post-attention\" normalization (`post_attention_layernorm` in the original `Llama4TextDecoderLayer`). Llama typically uses RMSNorm.\n",
    "\n",
    "# %%\n",
    "# Simplified RMSNorm Implementation\n",
    "\n",
    "\n",
    "class SimplifiedRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # Learnable gain parameter\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        # Calculate in float32 for stability\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        # Calculate variance (mean of squares) across the hidden dimension\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        # Normalize: input / sqrt(variance + epsilon)\n",
    "        hidden_states = hidden_states * \\\n",
    "            torch.rsqrt(variance + self.variance_epsilon)\n",
    "        # Apply learnable weight and cast back to original dtype\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "\n",
    "# Instantiate and apply the normalization\n",
    "post_attention_norm = SimplifiedRMSNorm(hidden_size, eps=rms_norm_eps)\n",
    "normalized_hidden_states = post_attention_norm(input_to_ffn_block)\n",
    "\n",
    "print(\"Shape after Post-Attention RMSNorm:\")\n",
    "print(f\"  normalized_hidden_states: {normalized_hidden_states.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: The Feed-Forward Network (MLP with Gated Linear Unit)\n",
    "#\n",
    "# The core of the FFN in Llama's dense layers is an MLP using a gated mechanism, often referred to as SwiGLU (SiLU Gated Linear Unit). It consists of three linear projections:\n",
    "#\n",
    "# 1.  **`gate_proj`:** Projects the input to the `intermediate_size`.\n",
    "# 2.  **`up_proj`:** Also projects the input to the `intermediate_size`.\n",
    "# 3.  **`down_proj`:** Projects the result back down to the `hidden_size`.\n",
    "#\n",
    "# The calculation is: `down_proj( F.silu(gate_proj(x)) * up_proj(x) )`\n",
    "#\n",
    "# - The `gate_proj` output is passed through an activation function (SiLU/Swish).\n",
    "# - This activated gate is element-wise multiplied by the `up_proj` output.\n",
    "# - The result is then projected back to the original hidden dimension by `down_proj`.\n",
    "\n",
    "# %%\n",
    "# Define FFN layers\n",
    "gate_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "up_proj = nn.Linear(hidden_size, intermediate_size, bias=ffn_bias)\n",
    "down_proj = nn.Linear(intermediate_size, hidden_size, bias=ffn_bias)\n",
    "\n",
    "# Define the activation function (SiLU/Swish)\n",
    "# ACT2FN could be used here, but for simplicity, we directly use nn.SiLU\n",
    "if hidden_act == \"silu\":\n",
    "    activation_fn = nn.SiLU()\n",
    "else:\n",
    "    # Add other activations if needed, otherwise raise error\n",
    "    raise NotImplementedError(\n",
    "        f\"Activation {hidden_act} not implemented in this example.\")\n",
    "\n",
    "# Apply the FFN layers to the *normalized* hidden states\n",
    "gate_output = gate_proj(normalized_hidden_states)\n",
    "up_output = up_proj(normalized_hidden_states)\n",
    "\n",
    "# Apply activation to the gate and perform element-wise multiplication\n",
    "activated_gate = activation_fn(gate_output)\n",
    "gated_result = activated_gate * up_output\n",
    "\n",
    "# Apply the final down projection\n",
    "ffn_output = down_proj(gated_result)\n",
    "\n",
    "print(\"\\nShapes within FFN:\")\n",
    "# (batch, seq_len, intermediate_size)\n",
    "print(f\"  gate_output: {gate_output.shape}\")\n",
    "# (batch, seq_len, intermediate_size)\n",
    "print(f\"  up_output: {up_output.shape}\")\n",
    "# (batch, seq_len, intermediate_size)\n",
    "print(f\"  gated_result: {gated_result.shape}\")\n",
    "print(f\"  ffn_output: {ffn_output.shape}\")   # (batch, seq_len, hidden_size)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4: Residual Connection\n",
    "#\n",
    "# Similar to the attention block, a residual connection is used around the FFN block. The output of the FFN (`ffn_output`) is added to the input that went *into* the FFN block (i.e., the output of the attention block + its residual, stored here as `input_to_ffn_block`).\n",
    "\n",
    "# %%\n",
    "# Add the FFN output to the input of the FFN block (before normalization)\n",
    "final_output = input_to_ffn_block + ffn_output\n",
    "\n",
    "print(\"\\nShape after FFN Residual Connection:\")\n",
    "# Should be (batch, seq_len, hidden_size)\n",
    "print(f\"  final_output: {final_output.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Putting it Together (Simplified Llama4 FFN Block)\n",
    "#\n",
    "# Let's combine the normalization and MLP steps into a simplified module. Note that the residual connection is typically handled *outside* this specific module in the main `DecoderLayer`.\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "class SimplifiedLlama4FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.hidden_act = config['hidden_act']\n",
    "        self.ffn_bias = config['ffn_bias']\n",
    "        self.rms_norm_eps = config['rms_norm_eps']\n",
    "\n",
    "        # Normalization Layer (applied before MLP)\n",
    "        self.norm = SimplifiedRMSNorm(self.hidden_size, eps=self.rms_norm_eps)\n",
    "\n",
    "        # MLP Layers\n",
    "        self.gate_proj = nn.Linear(\n",
    "            self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.up_proj = nn.Linear(\n",
    "            self.hidden_size, self.intermediate_size, bias=self.ffn_bias)\n",
    "        self.down_proj = nn.Linear(\n",
    "            self.intermediate_size, self.hidden_size, bias=self.ffn_bias)\n",
    "\n",
    "        # Activation\n",
    "        if self.hidden_act == \"silu\":\n",
    "            self.activation_fn = nn.SiLU()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Activation {self.hidden_act} not implemented.\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 1. Apply pre-FFN normalization\n",
    "        normalized_states = self.norm(hidden_states)\n",
    "\n",
    "        # 2. Apply MLP (SwiGLU)\n",
    "        gate = self.gate_proj(normalized_states)\n",
    "        up = self.up_proj(normalized_states)\n",
    "        down = self.down_proj(self.activation_fn(gate) * up)\n",
    "\n",
    "        # This module returns *only* the MLP output.\n",
    "        # The residual connection is applied outside.\n",
    "        return down\n",
    "\n",
    "\n",
    "# Instantiate and run the simplified module\n",
    "ffn_config_dict = {\n",
    "    'hidden_size': hidden_size,\n",
    "    'intermediate_size': intermediate_size,\n",
    "    'hidden_act': hidden_act,\n",
    "    'ffn_bias': ffn_bias,\n",
    "    'rms_norm_eps': rms_norm_eps,\n",
    "}\n",
    "\n",
    "simplified_ffn_module = SimplifiedLlama4FFN(ffn_config_dict)\n",
    "\n",
    "# Run forward pass using the module\n",
    "# Input is the state *before* the norm\n",
    "mlp_output_from_module = simplified_ffn_module(input_to_ffn_block)\n",
    "\n",
    "# Apply the residual connection externally\n",
    "final_output_from_module = input_to_ffn_block + mlp_output_from_module\n",
    "\n",
    "print(\"\\nOutput shape from simplified FFN module (before residual):\",\n",
    "      mlp_output_from_module.shape)\n",
    "print(\"Output shape after external residual connection:\",\n",
    "      final_output_from_module.shape)\n",
    "# Verify that the manual calculation matches the module output (should be very close)\n",
    "print(\"Outputs are close:\", torch.allclose(\n",
    "    final_output, final_output_from_module, atol=1e-6))\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Conclusion\n",
    "#\n",
    "# The Llama 4 Feed-Forward Network block typically consists of:\n",
    "# 1.  **Pre-Normalization:** An RMSNorm layer applied to the output of the previous (attention + residual) block.\n",
    "# 2.  **Gated MLP (SwiGLU):** Two linear layers projecting to an intermediate dimension, combined using an activation (SiLU) and element-wise multiplication, followed by a projection back to the hidden dimension.\n",
    "# 3.  **Residual Connection:** The output of the MLP is added back to the input of the normalization layer.\n",
    "#\n",
    "# This structure provides the necessary non-linearity and processing power for each token position within the transformer layer.\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
